{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests #request package to retrieve content from a website\n",
    "from bs4 import BeautifulSoup\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definizione di due funzioni, rispettivamente per estrarre le domande e le risposte.\n",
    "Per arrivare a queste conclusioni è stata fatta un'analisi preliminare del codice html delle pagine web del Ministero della salute contenenti domande e risposte (http://www.salute.gov.it/portale/nuovocoronavirus/archivioFaqNuovoCoronavirus.jsp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funzione per l'estrazione delle domande\n",
    "def extract_questions(soup):\n",
    "    questions = list()\n",
    "    for q in soup.find_all('strong'):\n",
    "        # controllo che mi serve per prendere effettivamente\n",
    "        # solo le domande\n",
    "        if (q.parent.name == 'button'): \n",
    "            s = q.string\n",
    "            questions.append(s.strip())\n",
    "    return (questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funzione per l'estrazione delle risposte\n",
    "def extract_answers(soup):\n",
    "    answers = list()\n",
    "    for div in soup.find_all('div', class_='collapse-body'):\n",
    "        # trovo tutti i link nel div\n",
    "        markup = div.find_all('a')\n",
    "        for a in markup:\n",
    "            # se il div ha qualche link\n",
    "            if (a):\n",
    "                if (a.get('href')[0] == \"/\"):\n",
    "                    base_url = 'http://www.salute.gov.it'\n",
    "                    link = base_url + a.get('href')\n",
    "                else:\n",
    "                    link = a.get('href')\n",
    "                # cambio il testo contenuto tra <a> e </a>\n",
    "                # con il link stesso\n",
    "                a.string = a.string + \"(\" + link + \")\"\n",
    "        # a questo punto estraggo il testo (che contiene i link)\n",
    "        # strip = True mi serve per pulirlo\n",
    "        ans = div.get_text(\" \", strip=True)\n",
    "        answers.append(ans)\n",
    "    return(answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questa funzione è necessaria per estrapolare le risposte di una pagina web che presenta un codice leggermente diverso dalle altre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# altra funzione per l'estrazione delle risposte\n",
    "def extract_answers2(soup):\n",
    "    answers = list()\n",
    "    for div in soup.find_all('div', class_='collapse-body'):\n",
    "        # trovo tutti i link nel div\n",
    "        markup = div.find_all('a')\n",
    "        for a in markup:\n",
    "            # se il div ha qualche link\n",
    "            if (a):\n",
    "                if (a.get('href')[0] == \"/\"):\n",
    "                    base_url = 'http://www.salute.gov.it'\n",
    "                    link = base_url + a.get('href')\n",
    "                else:\n",
    "                    link = a.get('href')\n",
    "                # cambio il testo contenuto tra <a> e </a>\n",
    "                # con il link stesso\n",
    "                if(a.string != None):\n",
    "                    a.string = a.string + \"(\" + link + \")\"\n",
    "        # a questo punto estraggo il testo (che contiene i link)\n",
    "        # strip = True mi serve per pulirlo\n",
    "        ans = div.get_text(\" \", strip=True)\n",
    "        answers.append(ans)\n",
    "    return(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funzione per estrarre i link delle pagine di cui effettuare lo scraping \n",
    "# delle domande e risposte\n",
    "def extract_link(soup):\n",
    "    links = list()\n",
    "    # url da integrare con quello presente nell'href\n",
    "    base_url = 'http://www.salute.gov.it'\n",
    "    # <ul> con class_='nav navgoco' è una delle liste che contiene gli url che voglio\n",
    "    # di queste <ul> ce ne sono due e io sono interessata alla seconda, quindi agisco\n",
    "    # alla seconda iterazione (quando i = 1)\n",
    "    i = 0\n",
    "    for ul in soup.find_all('ul', class_ = 'nav navgoco'):\n",
    "        if (i == 1):\n",
    "            markup = ul.find_all('a')\n",
    "            for a in markup:\n",
    "                l = base_url+a.get('href')\n",
    "                links.append(l)\n",
    "        i += 1\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fornisco un unico url di partenza\n",
    "url = 'http://www.salute.gov.it/portale/nuovocoronavirus/dettaglioFaqNuovoCoronavirus.jsp?lingua=italiano&id=231'\n",
    "soup = BeautifulSoup(requests.get(url).text)\n",
    "# estraggo i link delle pagine\n",
    "links = extract_link(soup)\n",
    "\n",
    "q_a = dict()\n",
    "for l in links:\n",
    "    url = l\n",
    "    # se l'url in questione è quello indicato,\n",
    "    # applico la seconda funzione per l'estrazione delle risposte\n",
    "    if url == 'http://www.salute.gov.it/portale/nuovocoronavirus/dettaglioFaqNuovoCoronavirus.jsp?lingua=italiano&id=237':\n",
    "        soup = BeautifulSoup(requests.get(url).text)\n",
    "        questions = extract_questions(soup)\n",
    "        answers = extract_answers2(soup)\n",
    "    else:\n",
    "        soup = BeautifulSoup(requests.get(url).text)\n",
    "        questions = extract_questions(soup)\n",
    "        answers = extract_answers(soup)\n",
    "    # metto tutto dentro una struttura dati (dizionario)\n",
    "    for q, a in zip (questions, answers):\n",
    "        q_a[q] = (a, url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "q_a = dizionario che ha per chiave la domanda e come valore una tupla. Il primo elemento della tupla è la risposta alla domanda e il secondo elemento è il link dal quale le informazioni sono state estratte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(q_a) # numero di FAQ in nostro possesso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# salvataggio della struttura dati in un file esterno\n",
    "with open('q&a.json', 'w') as json_file:\n",
    "    json.dump(q_a, json_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
